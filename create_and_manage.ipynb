{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation gestion de reçources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "    \n",
    "ws = Workspace.create(name='aml-workspace', \n",
    "                    subscription_id='123456-abc-123...',\n",
    "                    resource_group='aml-resources',\n",
    "                    create_resource_group=True,\n",
    "                    location='eastus'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réutiliser workspace deja crée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.get(name=\"Preparation-Flo-AI102-Clermont\",\n",
    "                   subscription_id= \"0252a218-2d27-4d77-a4f1-b638272c95e0\",\n",
    "                   resource_group=\"cloud-shell-storage-westeurope\")\n",
    "\n",
    "\n",
    "\n",
    "# pour ecrire le fichier config\n",
    "#  ws.write_config(path=\"./file-path\", file_name=\"ws_config.json\")\n",
    "# le fichier config peut aussi etre recuperé sur le portail azure (dans le workspace cliquer )\n",
    "\"\"\"\n",
    " le  fichier config sera du ytpe :\n",
    " {\n",
    "    \"subscription_id\": \"<subscription-id>\",\n",
    "    \"resource_group\": \"<resource-group>\",\n",
    "    \"workspace_name\": \"<workspace-name>\"\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si utilisation d'un fichier config pour charger le workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparation-AI102-Florian loaded\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, \"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "# create an experiment variable\n",
    "experiment = Experiment(workspace = ws, name = \"my-experiment\")\n",
    "\n",
    "# start the experiment\n",
    "run = experiment.start_logging()\n",
    "\n",
    "# experiment code goes here\n",
    "##########################\n",
    "##########################\n",
    "# load the dataset and count the rows\n",
    "data = pd.read_csv('data.csv')\n",
    "row_count = (len(data))\n",
    "# Log the row count\n",
    "run.log('observations', row_count)\n",
    "\n",
    "\n",
    "# end the experiment\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut utiliser:\n",
    "\n",
    "    log : enregistrez une seule valeur nommée.\n",
    "    log_list : enregistrez une liste nommée de valeurs.\n",
    "    log_row : enregistrez une ligne avec plusieurs colonnes.\n",
    "    log_table : enregistrez un dictionnaire sous forme de table.\n",
    "    log_image : enregistrez un fichier image ou un tracé.\n",
    "\n",
    "\n",
    "\n",
    "Pour log tout un tas de truc ... entrainement models etc : utiliser  du mlflow [mlflow for azure](https://learn.microsoft.com/fr-fr/azure/machine-learning/how-to-log-view-metrics?tabs=interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupération et affichage des métriques journalisées\n",
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()\n",
    "\n",
    "#  récupérer les métriques en tant que json\n",
    "import json\n",
    "metrics = run.get_metrics()\n",
    "print(json.dumps(metrics, indent=2))\n",
    "\n",
    "# Pour transferer les fichiers de journaux de la VM ou a lieu l'experience \n",
    "# vers le dossier de sorties de l’exécution(independant de la VM)\n",
    "run.upload_file(name='outputs/sample.csv', path_or_stream='./sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un \"script d’expérience\" = un **objet Run** + context dans un fichier .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voici un script d'experience :\n",
    "# on l'appelera experiment.py\n",
    "\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "# load the diabetes dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "# Count the rows and log the result\n",
    "row_count = (len(data))\n",
    "run.log('observations', row_count)\n",
    "# Save a sample of the data\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "data.sample(100).to_csv(\"outputs/sample.csv\", index=False, header=True)\n",
    "# Complete the run\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On execute un script (.py contenant l'objet run) dans une **Experience** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, ScriptRunConfig\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=experiment_folder,\n",
    "                                script='experiment.py') \n",
    "\n",
    "# submit the experiment\n",
    "experiment = Experiment(workspace = ws, name = 'my-experiment')\n",
    "run = experiment.submit(config=script_config)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avec name='outputs/*' on enregistre des choses dans le  run's output folder\n",
    "\n",
    "run.log_image(name='outputs/label distribution', plot=fig) # fig est mon image\n",
    "run.upload_file(name='outputs/sample.csv', path_or_stream='./sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trois manieres de voir les details de l'experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec un widget dans le notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En regardant les metriques et les fichiers enregistrés dans le dossier outputs pendant le run\n",
    "\n",
    "(On pourrait ecrir les metriques dans un fichier json aussi si on voulait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Get logged metrics\n",
    "print(\"Metrics:\")\n",
    "metrics = run.get_metrics()\n",
    "for metric_name in metrics:\n",
    "    print(metric_name, \":\", metrics[metric_name])\n",
    "\n",
    "# Get output files\n",
    "print(\"\\nFiles:\")\n",
    "files = run.get_file_names()\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En telechargeant le dossier outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "download_folder = 'downloaded-files'\n",
    "# Download files in the \"outputs\" folder\n",
    "run.download_files(prefix='outputs', output_directory=download_folder)\n",
    "\n",
    "# Verify the files have been downloaded\n",
    "for root, directories, filenames in os.walk(download_folder): \n",
    "    for filename in filenames:  \n",
    "        print (os.path.join(root,filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi telecharger tout les log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "log_folder = 'downloaded-logs'\n",
    "# Download all files\n",
    "run.get_all_logs(destination=log_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi avoir des details sur l'experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_details_with_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jusque la on a executé des experiments depuis notre notebook  \n",
    "Mais on peut ecrire des experiments dans des fichiers .py et les appeler  \n",
    "Ici on a ecrit un script et les données qui vont avec, on les a mis dans le dossier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_folder='chemin vers le dossier'\n",
    "\n",
    "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "from azureml.widgets import RunDetails\n",
    "# Create a Python environment for the experiment (from a .yml file)\n",
    "env = Environment.from_conda_specification(\"experiment_env\", \"environment.yml\")\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=experiment_folder,\n",
    "                                script='diabetes_experiment.py',\n",
    "                                environment=env,\n",
    "                                docker_runtime_config=DockerConfiguration(use_docker=True))\n",
    "\n",
    "# submit the experiment\n",
    "experiment = Experiment(workspace=ws, name='mslearn-diabetes')\n",
    "run = experiment.submit(config=script_config)\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour plus de details voir \n",
    "[Notebook4 officiel azure](./mslearn-dp100/04%20-%20Run%20Experiments.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour utiliser mlflow:  \n",
    "pip show azureml-mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "# Set the MLflow tracking URI to the workspace\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "\n",
    "# Create an Azure ML experiment in your workspace\n",
    "experiment = Experiment(workspace=ws, name='mslearn-diabetes-mlflow')\n",
    "mlflow.set_experiment(experiment.name)\n",
    "\n",
    "# start the MLflow experiment\n",
    "with mlflow.start_run():\n",
    "    print(\"Starting experiment:\", experiment.name)\n",
    "    # Load data\n",
    "    data = pd.read_csv('data/diabetes.csv')\n",
    "    # Count the rows and log the result\n",
    "    row_count = (len(data))\n",
    "    mlflow.log_metric('observations', row_count)\n",
    "    print(\"Run complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour plus de details voir \n",
    "[Notebook4 officiel azure](./mslearn-dp100/04%20-%20Run%20Experiments.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrainer un model**  \n",
    "Etape 1 faire un script d'entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    -Etape 1 : faire un script d'entrainement\n",
    "    -Etape 2 : soumettre le  script d'entrainement\n",
    "    -Etape 3 : Enregistrer/inscrire(=versionner le model) dans le workspace\n",
    "Pour plus de details voir \n",
    "[Notebook5 officiel azure](./mslearn-dp100/05%20-%20Train%20Models.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Telecharger localement un model du run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"run\" is a reference to a completed experiment run\n",
    "# List the files generated by the experiment\n",
    "for file in run.get_file_names():\n",
    "    print(file)\n",
    "# Download a named file\n",
    "run.download_file(name='outputs/model.pkl', output_file_path='model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour un Model enregistré en local\n",
    "from azureml.core import Model\n",
    "model = Model.register(workspace=ws,\n",
    "                       model_name='classification_model',\n",
    "                       model_path='model.pkl', # local path\n",
    "                       description='A classification model',\n",
    "                       tags={'data-format': 'CSV'},\n",
    "                       model_framework=Model.Framework.SCIKITLEARN,\n",
    "                       model_framework_version='0.20.3')\n",
    "\n",
    "\n",
    "# Pour un model enregistré au cours d'un run:\n",
    "run.register_model( model_name='classification_model',\n",
    "                    model_path='outputs/model.pkl', # run outputs path\n",
    "                    description='A classification model',\n",
    "                    tags={'data-format': 'CSV'},\n",
    "                    model_framework=Model.Framework.SCIKITLEARN,\n",
    "                    model_framework_version='0.20.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour avoir la liste des models inscrits:\n",
    "from azureml.core import Model\n",
    "for model in Model.list(ws):\n",
    "    # Get model name and auto-generated version\n",
    "    print(model.name, 'version:', model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore\n",
    "# Get the default datastore  - usually it's the blobspace \n",
    "default_ds = ws.get_default_datastore()\n",
    "#pour voir la liste des datastores:\n",
    "for ds_name in ws.datastores:\n",
    "    print(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore\n",
    "ws = Workspace.from_config()\n",
    "# Register a new datastore\n",
    "blob_ds = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                                  datastore_name='blob_data', \n",
    "                                                  container_name='data_container',\n",
    "                                                  account_name='az_store_acct',\n",
    "                                                  account_key='123456abcde789…')\n",
    "\n",
    "blob_store = Datastore.get(ws, datastore_name='blob_data')\n",
    "# Et pour choisir un nouveau datastore par default:\n",
    "ws.set_default_datastore('blob_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Envoyer des données au storage (ici blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "default_ds = ws.get_default_datastore()\n",
    "Dataset.File.upload_directory(src_dir='data',   # Chemin local \n",
    "                              target=DataPath(default_ds, 'diabetes-data/')  # Chemin az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Créer et inscrir un jeu de données**\n",
    "\n",
    "    - Tabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a tabular dataset from the path on the datastore (this may take a short while)\n",
    "tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-data/*.csv'))\n",
    "# Register the tabular dataset\n",
    "tab_data_set = tab_data_set.register(workspace=ws, name='csv_table')\n",
    "# Register with more details\n",
    "tab_data_set = tab_data_set.register(workspace=ws, \n",
    "                                    name='diabetes dataset',\n",
    "                                    description='diabetes data',\n",
    "                                    tags = {'format':'CSV'},\n",
    "                                    create_new_version=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - De Fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "file_data_set = Dataset.File.from_files(path=(default_ds, 'diabetes-data/*.csv'))\n",
    "# Register\n",
    "file_data_set = file_data_set.register(workspace=ws, name='csv_file')\n",
    "# Register more details\n",
    "file_data_set = file_data_set.register(workspace=ws,\n",
    "                                        name='diabetes file dataset',\n",
    "                                        description='diabetes files',\n",
    "                                        tags = {'format':'CSV'},\n",
    "                                        create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrive Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version is by default the latest\n",
    "img_ds = Dataset.get_by_name(workspace=ws,name='img_files', version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passer des données tabulaires à un script d'entrainement\n",
    "Il y a deux manieres de faire:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Utiliser un argument de script  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScriptRunConfig\n",
    "env = Environment('my_env')\n",
    "packages = CondaDependencies.create(conda_packages=['pip'],\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', tab_ds], \n",
    "                                environment=env) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fichier script.py\n",
    "from azureml.core import Run, Dataset\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='dataset_id')\n",
    "args = parser.parse_args()\n",
    "\n",
    "run = Run.get_context()\n",
    "ws = run.experiment.workspace\n",
    "dataset = Dataset.get_by_id(ws, id=args.dataset_id)\n",
    "data = dataset.to_pandas_dataframe()\n",
    "\n",
    "# Cette façon retrouve le dataset a partir de son ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Utiliser une entrée nommée\n",
    "(le nom du jeu de données qui sera recuperé grace au contexte ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment('my_env')\n",
    "packages = CondaDependencies.create(conda_packages=['pip'],\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', tab_ds.as_named_input('my_dataset')],\n",
    "                                environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='ds_id')\n",
    "args = parser.parse_args()\n",
    "\n",
    "run = Run.get_context()\n",
    "dataset = run.input_datasets['my_dataset']\n",
    "data = dataset.to_pandas_dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passer des fichiers à un script d'entrainement\n",
    "On doit specifier le mode de transmition des fichiers:  \n",
    "     - **as_download()**\n",
    "telecharge les données sur la cible de calcul  \n",
    "     - **as_mount()**\n",
    "stream les données du storage vers la cible de calcul  \n",
    "\n",
    "\n",
    "Il y a deux manieres de faire:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Avec un argument de script  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scriptrunconfig\n",
    "env = Environment('my_env')\n",
    "packages = CondaDependencies.create(conda_packages=['pip'],\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', file_ds.as_download()],\n",
    "                                environment=env) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script.py\n",
    "from azureml.core import Run\n",
    "import glob\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='ds_ref')\n",
    "args = parser.parse_args()\n",
    "run = Run.get_context()\n",
    "\n",
    "imgs = glob.glob(args.ds_ref + \"/*.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Utiliser une entrée nommée\n",
    "(le nom du jeu de données qui sera recuperé grace au contexte ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scriptrunconfig\n",
    "env = Environment('my_env')\n",
    "packages = CondaDependencies.create(conda_packages=['pip'],\n",
    "                                    pip_packages=['azureml-defaults',\n",
    "                                                  'azureml-dataprep[pandas]'])\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                arguments=['--ds', file_ds.as_named_input('my_ds').as_download()],\n",
    "                                environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script.py\n",
    "from azureml.core import Run\n",
    "import glob\n",
    "\n",
    "parser.add_argument('--ds', type=str, dest='ds_ref')\n",
    "args = parser.parse_args()\n",
    "run = Run.get_context()\n",
    "\n",
    "dataset = run.input_datasets['my_ds']\n",
    "imgs= glob.glob(dataset + \"/*.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environements virtuels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée un fichier .yml :\n",
    "\n",
    "     - exemple d'un fichier conda.yml:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "name: py_env\n",
    "dependencies:\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - scikit-learn\n",
    "  - pip:\n",
    "    - azureml-defaults\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation de l'environnement **sur base du fichier .yml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "env = Environment.from_conda_specification(name='training_environment',\n",
    "                                           file_path='./conda.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation de l'environnement **en specifiant les packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "env = Environment('training_environment')\n",
    "deps = CondaDependencies.create(conda_packages=['scikit-learn','pandas','numpy'],\n",
    "                                pip_packages=['azureml-defaults'])\n",
    "env.python.conda_dependencies = deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise un env crée\n",
    "Par default **cet env sera dans un container Docker**  \n",
    "mais on peut passer use_docker=False\n",
    "alors l'environnement sera crée directement sur la cible de calcul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, ScriptRunConfig\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "\n",
    "docker_config = DockerConfiguration(use_docker=True)\n",
    "script_config = ScriptRunConfig(source_directory='my_folder',\n",
    "                                script='my_script.py',\n",
    "                                environment=env,\n",
    "                                docker_runtime_config=docker_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut aussi modifier l'image docker de base avec \n",
    "env.docker.base_image='my-base-image'\n",
    "env.docker.base_image_registry='myregistry.azurecr.io/myimage\n",
    "\n",
    "# Ou meme une image construit a la minute\n",
    "env.docker.base_image = None\n",
    "env.docker.base_dockerfile = './Dockerfile'\n",
    "\n",
    "# On peut aussi forcer l'utilisation d'une install python des librairies   ***presente dans le conteneur***\n",
    "\n",
    "env.python.user_managed_dependencies=True\n",
    "env.python.interpreter_path = '/opt/miniconda/bin/python'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enregistrer un env pour le réutiliuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrement\n",
    "env.register(workspace=ws)\n",
    "# lister les env existants\n",
    "from azureml.core import Environment\n",
    "env_names = Environment.list(workspace=ws)\n",
    "for env_name in env_names:\n",
    "    print('Name:',env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperation et réutilisation\n",
    "from azureml.core import Environment, ScriptRunConfig\n",
    "training_env = Environment.get(workspace=ws, name='training_environment')\n",
    "script_config = ScriptRunConfig(source_directory='my_folder',\n",
    "                                script='my_script.py',\n",
    "                                environment=training_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer Cible de calcul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cluster de calcul**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici on crée un cluster de calcul :\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "# Specify a name for the compute (unique within the workspace)\n",
    "compute_name = 'aml-cluster'\n",
    "# Define compute configuration\n",
    "compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2',\n",
    "                                                       min_nodes=0, max_nodes=4,\n",
    "                                                       vm_priority='dedicated')\n",
    "# Create the compute\n",
    "aml_cluster = ComputeTarget.create(ws, compute_name, compute_config)\n",
    "aml_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "# Specify a name for the compute (unique within the workspace)\n",
    "compute_name = 'db_cluster'\n",
    "# Define configuration for existing Azure Databricks cluster\n",
    "db_workspace_name = 'db_workspace'\n",
    "db_resource_group = 'db_resource_group'\n",
    "db_access_token = '1234-abc-5678-defg-90...'\n",
    "db_config = DatabricksCompute.attach_configuration(resource_group=db_resource_group,\n",
    "                                                   workspace_name=db_workspace_name,\n",
    "                                                   access_token=db_access_token)\n",
    "\n",
    "# Atache une cible de calcul\n",
    "databricks_compute = ComputeTarget.attach(ws, compute_name, db_config)\n",
    "databricks_compute.wait_for_completion(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On a donc \n",
    "ComputeTarget.create(ws, compute_name, compute_config)\n",
    "# et\n",
    "ComputeTarget.attach(ws, compute_name, db_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    aml_cluster = ComputeTarget(workspace=ws, name=compute_name)\n",
    "    print('Found existing cluster.')\n",
    "except ComputeTargetException:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choix de la cible de calcul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, ScriptRunConfig\n",
    "\n",
    "compute_name = 'aml-cluster'\n",
    "training_env = Environment.get(workspace=ws, name='training_environment')\n",
    "\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                environment=training_env,\n",
    "                                compute_target=compute_name) # ON passe le nom de la cible de calcul\n",
    "\n",
    "\n",
    "    \n",
    "compute_name = \"aml-cluster\"\n",
    "training_cluster = ComputeTarget(workspace=ws, name=compute_name)\n",
    "script_config = ScriptRunConfig(source_directory='my_dir',\n",
    "                                script='script.py',\n",
    "                                environment=training_env,\n",
    "                                compute_target=training_cluster) # Sinon on passe un Compute Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "     - PythonScriptStep : Exécute un script Python spécifié.\n",
    "     - DataTransferStep : Utilise Azure Data Factory pour copier des données entre des magasins de données.\n",
    "     - DatabricksStep : Exécute un notebook, un script ou un fichier JAR compilé sur un cluster Databricks.\n",
    "     - AdlaStep : Exécute une tâche U-SQL dans Azure Data Lake Analytics.\n",
    "     - ParallelRunStep : Exécute un script Python comme tâche distribuée sur plusieurs nœuds de calcul.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transmition des données entre les etapes du pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "\n",
    "# Get a dataset for the initial data\n",
    "raw_ds = Dataset.get_by_name(ws, 'raw_dataset')\n",
    "\n",
    "# Define a PipelineData object to pass data between steps\n",
    "data_store = ws.get_default_datastore()\n",
    "prepped_data = OutputFileDatasetConfig('prepped')     ########### C'est cet objet qui défini le nom du dossier temp pour la transmition des données    \n",
    "\n",
    "# Step to run a Python script\n",
    "step1 = PythonScriptStep(name = 'prepare data',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         # Script arguments include PipelineData\n",
    "                         arguments = ['--raw-ds', raw_ds.as_named_input('raw_data'),\n",
    "                                      '--out_folder', prepped_data])  ### on le passe comme argument\n",
    "\n",
    "# Step to run an estimator\n",
    "step2 = PythonScriptStep(name = 'train model',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'train_model.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         # Pass as script argument\n",
    "                         arguments=['--training-data', prepped_data.as_input()])  ### on le passe comme argument\n",
    "\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core import Experiment\n",
    "# Construct the pipeline\n",
    "train_pipeline = Pipeline(workspace = ws, steps = [step1,step2])\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace = ws, name = 'training-pipeline')\n",
    "pipeline_run = experiment.submit(train_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code in data_prep.py (un des deux script)\n",
    "from azureml.core import Run\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Get arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--raw-ds', type=str, dest='raw_dataset_id')\n",
    "\n",
    "#####\n",
    "#Ici a utilisé l'argparser pour\n",
    "# faire une reference à OutputFileDatasetConfig\n",
    "# et l'utiliser comme dossier local\n",
    "parser.add_argument('--out_folder', type=str, dest='folder')\n",
    "args = parser.parse_args()\n",
    "output_folder = args.folder\n",
    "#####\n",
    "\n",
    "\n",
    "# Get input dataset as dataframe\n",
    "raw_df = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
    "\n",
    "# code to prep data (in this case, just select specific columns)\n",
    "prepped_df = raw_df[['col1', 'col2', 'col3']]\n",
    "\n",
    "# Save prepped data to the PipelineData location\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = os.path.join(output_folder, 'prepped_data.csv')\n",
    "prepped_df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## long long pipelines and re-use of intermediary step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = PythonScriptStep(name = 'prepare data',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         runconfig = run_config,\n",
    "                         inputs=[raw_ds.as_named_input('raw_data')],\n",
    "                         outputs=[prepped_data],\n",
    "                         arguments = ['--folder', prepped_data],\n",
    "                         # Disable step reuse\n",
    "                         allow_reuse = True)   ### Choose true or false to re use\n",
    "\n",
    "# Choose true to re-use\n",
    "# Attention reuse => resultats obsolete si modif des données pas pris en compte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forcing all steps to run (and not look at rhe reuse configuration )\n",
    "pipeline_run = experiment.submit(train_pipeline, regenerate_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have created a pipeline, you can **publish** it to create a REST endpoint through which **the pipeline can be run on demand**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name='training_pipeline',\n",
    "                                          description='Model training pipeline',\n",
    "                                          version='1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can **call** the publish method **on a successful run of the pipeline**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent run of the pipeline\n",
    "pipeline_experiment = ws.experiments.get('training-pipeline')\n",
    "run = list(pipeline_experiment.get_runs())[0]\n",
    "\n",
    "# Publish the pipeline from the run\n",
    "published_pipeline = run.publish_pipeline(name='training_pipeline',\n",
    "                                          description='Model training pipeline',\n",
    "                                          version='1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also determine the URI of its **endpoint** like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initiate a published endpoint, you make an **HTTP request to its REST endpoint**,  \n",
    " passing an **authorization header** with a token for a service principal with permission to run the pipeline, **and a JSON payload** specifying the experiment name.  \n",
    " The pipeline is run asynchronously, so the response from a successful REST call includes the run ID. You can use this to track the run in Azure Machine Learning studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, the following Python code makes a REST request to run a pipeline and displays the returned run ID.\n",
    "import requests\n",
    "response = requests.post(rest_endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"run_training_pipeline\"})\n",
    "run_id = response.json()[\"Id\"]\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining parameters for a pipeline\n",
    "     - You must define parameters for a pipeline before publishing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "\n",
    "reg_param = PipelineParameter(name='reg_rate', default_value=0.01)   #########\n",
    "...\n",
    "step2 = PythonScriptStep(name = 'train model',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         # Pass parameter as script argument\n",
    "                         arguments=['--in_folder', prepped_data,\n",
    "                                    '--reg', reg_param],\n",
    "                         inputs=[prepped_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you publish a parameterized pipeline, you can pass parameter values in the JSON payload \n",
    "response = requests.post(rest_endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"run_training_pipeline\",\n",
    "                               \"ParameterAssignments\": {\"reg_rate\": 0.1}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling a pipeline for periodic interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "\n",
    "daily = ScheduleRecurrence(frequency='Day', interval=1)          ###########\n",
    "pipeline_schedule = Schedule.create(ws, name='Daily Training',\n",
    "                                        description='trains model every day',\n",
    "                                        pipeline_id=published_pipeline.id,\n",
    "                                        experiment_name='Training_Pipeline',\n",
    "                                        recurrence=daily)        ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triggering a pipeline run on data changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "from azureml.pipeline.core import Schedule\n",
    "\n",
    "training_datastore = Datastore(workspace=ws, name='blob_data')\n",
    "pipeline_schedule = Schedule.create(ws, name='Reactive Training',\n",
    "                                    description='trains model on data change',\n",
    "                                    pipeline_id=published_pipeline.id,\n",
    "                                    experiment_name='Training_Pipeline',\n",
    "                                    datastore=training_datastore,\n",
    "                                    path_on_datastore='data/training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un Autre Pipeline de A à Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import / Prepare Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "# Choose datastore to use\n",
    "default_ds = ws.get_default_datastore()\n",
    "# Uploader des fichiers locaux\n",
    "Dataset.File.upload_directory(src_dir='data', # local folder\n",
    "                              target=DataPath(default_ds, 'diabetes-data/')\n",
    "                              )\n",
    "# Creer un dataset tabulaire a partir des fichiers uploadés\n",
    "tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-data/*.csv'))\n",
    "# enregistrer le jeu de données tabulaire\n",
    "tab_data_set = tab_data_set.register(workspace=ws, \n",
    "                        name='diabetes dataset',\n",
    "                        description='diabetes data',\n",
    "                        tags = {'format':'CSV'},\n",
    "                        create_new_version=True)\n",
    "print('Dataset registered.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scriptes (un pour chaque étape du pipeline)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Script 1 - Preparation des données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/prep_diabetes.py\n",
    "# Import libraries\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from azureml.core import Run\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input-data\", type=str, dest='raw_dataset_id', help='raw dataset')\n",
    "parser.add_argument('--prepped-data', type=str, dest='prepped_data', default='prepped_data', help='Folder for results')\n",
    "args = parser.parse_args()\n",
    "save_folder = args.prepped_data\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the data (passed as an input dataset)\n",
    "print(\"Loading Data...\")\n",
    "diabetes = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
    "\n",
    "# Log raw row count\n",
    "row_count = (len(diabetes))\n",
    "run.log('raw_rows', row_count)\n",
    "\n",
    "# remove nulls\n",
    "diabetes = diabetes.dropna()\n",
    "\n",
    "# Normalize the numeric columns\n",
    "scaler = MinMaxScaler()\n",
    "num_cols = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree']\n",
    "diabetes[num_cols] = scaler.fit_transform(diabetes[num_cols])\n",
    "\n",
    "# Log processed rows\n",
    "row_count = (len(diabetes))\n",
    "run.log('processed_rows', row_count)\n",
    "\n",
    "# Save the prepped data\n",
    "print(\"Saving Data...\")\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "save_path = os.path.join(save_folder,'data.csv')\n",
    "diabetes.to_csv(save_path, index=False, header=True)\n",
    "\n",
    "# End the run\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scripte 2  -   Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/train_diabetes.py\n",
    "# Import libraries\n",
    "from azureml.core import Run, Model\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--training-data\", type=str, dest='training_data', help='training data')\n",
    "args = parser.parse_args()\n",
    "training_data = args.training_data\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the prepared data file in the training folder\n",
    "print(\"Loading Data...\")\n",
    "file_path = os.path.join(training_data,'data.csv')\n",
    "diabetes = pd.read_csv(file_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train adecision tree model\n",
    "print('Training a decision tree model...')\n",
    "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "# plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "# Plot the diagonal 50% line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# Plot the FPR and TPR achieved by our model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "run.log_image(name = \"ROC\", plot = fig)\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model in the outputs folder\n",
    "print(\"Saving model...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "model_file = os.path.join('outputs', 'diabetes_model.pkl')\n",
    "joblib.dump(value=model, filename=model_file)\n",
    "\n",
    "# Register the model\n",
    "print('Registering model...')\n",
    "Model.register(workspace=run.experiment.workspace,\n",
    "               model_path = model_file,\n",
    "               model_name = 'diabetes_model',\n",
    "               tags={'Training context':'Pipeline'},\n",
    "               properties={'AUC': np.float(auc), 'Accuracy': np.float(acc)})\n",
    "\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create/Get Compute target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = \"your-compute-cluster\"\n",
    "# Get \n",
    "pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "# Or create\n",
    "compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "pipeline_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write .yaml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/experiment_env.yml\n",
    "name: experiment_env\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- ipykernel\n",
    "- matplotlib\n",
    "- pandas\n",
    "- pip\n",
    "- pip:\n",
    "  - azureml-defaults\n",
    "  - pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create/Get env + RunConfig**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "# Cretate env from .yml\n",
    "experiment_env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/experiment_env.yml\")\n",
    "# Register the environment \n",
    "experiment_env.register(workspace=ws)\n",
    "registered_env = Environment.get(ws, 'experiment_env')\n",
    "# Create runconfig ( to use accross the pipelines)\n",
    "pipeline_run_config = RunConfiguration()\n",
    "pipeline_run_config.target = pipeline_cluster\n",
    "pipeline_run_config.environment = registered_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the Pipeline (steps)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "# Get the training dataset\n",
    "diabetes_ds = ws.datasets.get(\"diabetes dataset\")\n",
    "# Create an OutputFileDatasetConfig (temporary Data Reference) for data passed from step 1 to step 2\n",
    "prepped_data = OutputFileDatasetConfig(\"prepped_data\")\n",
    "# Step 1, Run the data prep script\n",
    "prep_step = PythonScriptStep(name = \"Prepare Data\",\n",
    "                                source_directory = experiment_folder,\n",
    "                                script_name = \"prep_diabetes.py\",\n",
    "                                arguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n",
    "                                             '--prepped-data', prepped_data],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "# Step 2, run the training script\n",
    "train_step = PythonScriptStep(name = \"Train and Register Model\",\n",
    "                                source_directory = experiment_folder,\n",
    "                                script_name = \"train_diabetes.py\",\n",
    "                                arguments = ['--training-data', prepped_data.as_input()],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build the Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.widgets import RunDetails\n",
    "# Construct the pipeline\n",
    "pipeline_steps = [prep_step, train_step]\n",
    "pipeline = Pipeline(workspace=ws, steps=pipeline_steps,)\n",
    "print(\"Pipeline is built.\")\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace=ws, name = 'mslearn-diabetes-pipeline')\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "RunDetails(pipeline_run).show()\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrive scores and metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in pipeline_run.get_children():\n",
    "    print(run.name, ':')\n",
    "    metrics = run.get_metrics()\n",
    "    for metric_name in metrics:\n",
    "        print('\\t',metric_name, \":\", metrics[metric_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline sucessfull=> le model est register dans  le workspace à la fin du script d'entrainement (ecrit dans le script)  \n",
    "**Check Registered  model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Publier le Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publie from the run as a REST Service\n",
    "published_pipeline = pipeline_run.publish_pipeline(          \n",
    "    name=\"diabetes-training-pipeline\", description=\"Trains diabetes model\", version=\"1.0\")\n",
    "\n",
    "# Recuperer Endpoint\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rappeler le Endpoint du pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autenthification\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "auth_header = interactive_auth.get_authentication_header()\n",
    "print(\"Authentication header ready.\")\n",
    "\n",
    "# Appel au Endpoint \n",
    "import requests\n",
    "experiment_name = 'mslearn-diabetes-pipeline'\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=auth_header, \n",
    "                         json={\"ExperimentName\": experiment_name})\n",
    "run_id = response.json()[\"Id\"]\n",
    "run_id\n",
    "\n",
    "# Grace à ''run_id'' on peut voir le deroulement de notre pipeline \n",
    "from azureml.pipeline.core.run import PipelineRun\n",
    "published_pipeline_run = PipelineRun(ws.experiments[experiment_name], run_id)\n",
    "published_pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schedule Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "# Submit the Pipeline every Monday at 00:00 UTC\n",
    "recurrence = ScheduleRecurrence(frequency=\"Week\", interval=1, week_days=[\"Monday\"], time_of_day=\"00:00\")\n",
    "weekly_schedule = Schedule.create(ws, name=\"weekly-diabetes-training\", \n",
    "                                  description=\"Based on time\",\n",
    "                                  pipeline_id=published_pipeline.id, \n",
    "                                  experiment_name='mslearn-diabetes-pipeline', \n",
    "                                  recurrence=recurrence)\n",
    "# Voir les schedules\n",
    "schedules = Schedule.list(ws)\n",
    "\n",
    "# Check latest run :\n",
    "pipeline_experiment = ws.experiments.get('mslearn-diabetes-pipeline')\n",
    "latest_run = list(pipeline_experiment.get_runs())[0]\n",
    "latest_run.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relance déclanché par changement de données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "from azureml.pipeline.core import Schedule\n",
    "training_datastore = Datastore(workspace=ws, name='blob_data')\n",
    "pipeline_schedule = Schedule.create(ws, name='Reactive Training',\n",
    "                                    description='trains model on data change',\n",
    "                                    pipeline_id=published_pipeline.id,\n",
    "                                    experiment_name='Training_Pipeline',\n",
    "                                    datastore=training_datastore,\n",
    "                                    path_on_datastore='data/training')   ###### Dossier surveillé pour la relance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployer Pipelines - Temps Réel\n",
    "\n",
    "Sur:\n",
    "- calcul local\n",
    "- une instance de calcul Azure Machine Learning,\n",
    "- une instance ACI (Azure Container Instance),\n",
    "- cluster AKS (Azure Kubernetes Services),\n",
    "- une fonction Azure Functions\n",
    "- module Internet des objets (IoT)\n",
    "\n",
    "En uttilisant un **conteneur Docker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 - Inscrire Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "# Inscrire un model depuis disque local\n",
    "classification_model = Model.register(workspace=ws,\n",
    "                       model_name='classification_model',\n",
    "                       model_path='model.pkl', # local path\n",
    "                       description='A classification model')\n",
    "\n",
    "# Inscrire un model apres un run(entrainement)\n",
    "run.register_model( model_name='classification_model',\n",
    "                    model_path='outputs/model.pkl', # run outputs path\n",
    "                    description='A classification model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Script d'entrée**\n",
    "\n",
    "    init() : appelée lorsque le service est initialisé.\n",
    "    run(raw_data) : appelée lorsque de nouvelles données sont envoyées au service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dans un .py\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Called when the service is loaded\n",
    "def init():\n",
    "    global model\n",
    "    # Get the path to the registered model file and load it\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.pkl')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "# Called when a request is received\n",
    "def run(raw_data):\n",
    "    # Get the input data as a numpy array\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # Get a prediction from the model\n",
    "    predictions = model.predict(data)\n",
    "    # Return the predictions as any JSON serializable format\n",
    "    return predictions.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 - Créer un environnement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "service_env = Environment(name='service-env')\n",
    "python_packages = ['scikit-learn', 'numpy'] # whatever packages your entry script uses\n",
    "for package in python_packages:\n",
    "    service_env.python.conda_dependencies.add_pip_package(package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 - Combiner en un Inference Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "classifier_inference_config = InferenceConfig(source_directory = 'service_files',\n",
    "                                              entry_script=\"score.py\",\n",
    "                                              environment=service_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5 - Config de Déploiement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on veux utiliser AKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "\n",
    "cluster_name = 'aks-cluster'\n",
    "compute_config = AksCompute.provisioning_configuration(location='westeurop')\n",
    "production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "production_cluster.wait_for_completion(show_output=True)\n",
    "\n",
    "\n",
    "from azureml.core.webservice import AksWebservice\n",
    "classifier_deploy_config = AksWebservice.deploy_configuration(cpu_cores = 1,\n",
    "                                                              memory_gb = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on veux utiliser ACI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
    "#et Pour en Local : \n",
    "azureml.core.webservice.LocalWebservice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6 - Deployer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "model = ws.models['classification_model']\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name = 'classifier-service',\n",
    "                       models = [model],\n",
    "                       inference_config = classifier_inference_config,\n",
    "                       deployment_config = classifier_deploy_config,\n",
    "                       deployment_target = production_cluster)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "\n",
    "\n",
    "# Ou sinon \n",
    "service_name = \"diabetes-service\"\n",
    "service = Model.deploy(ws, service_name, [model], inference_config, deployment_config, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# An array of new data cases\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "# Call the web service, passing the input data\n",
    "response = service.run(input_data = json_data)###################  On envoi les données pour inference \n",
    "# Get the predictions\n",
    "predictions = json.loads(response)\n",
    "# Print the predicted class for each case.\n",
    "for i in range(len(x_new)):\n",
    "    print (x_new[i], predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authentification**\n",
    "en production  \n",
    "deux types d’authentification :\n",
    "\n",
    "    - Clé : les demandes sont authentifiées en spécifiant la clé associée au service.\n",
    "    - Jeton : les demandes sont authentifiées en fournissant un jeton JWT (JSON Web Token).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = service.scoring_uri\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utiliser le service dans un autre programme**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "# An array of new data cases\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "\n",
    "# Set the content type in the request headers\n",
    "request_headers = { 'Content-Type':'application/json' }\n",
    "# Call the service\n",
    "response = requests.post(url = endpoint,\n",
    "                         data = json_data,\n",
    "                         headers = request_headers)\n",
    "\n",
    "\n",
    "# Get the predictions from the JSON response\n",
    "predictions = json.loads(response.json())\n",
    "\n",
    "# Print the predicted class for each case.\n",
    "for i in range(len(x_new)):\n",
    "    print (x_new[i]), predictions[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.state)\n",
    "\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deployer sur un conteneur en local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "deployment_config = LocalWebservice.deploy_configuration(port=8890)\n",
    "service = Model.deploy(ws, 'test-svc', [model], inference_config, deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour debuger le service :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester le service\n",
    "print(service.run(input_data = json_data))\n",
    "\n",
    "######\n",
    "#Resolution des problemes\n",
    "###### \n",
    "service.reload()\n",
    "print(service.run(input_data = json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployer Pipelines - Inference de lots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 - Inscrire un Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model  Local \n",
    "from azureml.core import Model\n",
    "classification_model = Model.register(workspace=your_workspace,\n",
    "                                      model_name='classification_model',\n",
    "                                      model_path='model.pkl', # local path\n",
    "                                      description='A classification model')\n",
    "\n",
    "# Model entrainé lors d'un run\n",
    "run.register_model( model_name='classification_model',\n",
    "                    model_path='outputs/model.pkl', # run outputs path\n",
    "                    description='A classification model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Créer un  script de scoring**  \n",
    "Le service d’inférence de lot nécessite un script de scoring pour charger le modèle et l’utiliser pour prédire de nouvelles valeurs. Il doit inclure deux fonctions :\n",
    "\n",
    "    - init() : appelée quand le pipeline est initialisé.\n",
    "    - run(mini_batch) : appelée pour chaque lot de données à traiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from azureml.core import Model\n",
    "import joblib\n",
    "\n",
    "def init():\n",
    "    # Runs when the pipeline step is initialized\n",
    "    global model\n",
    "\n",
    "    # load the model\n",
    "    model_path = Model.get_model_path('classification_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(mini_batch):\n",
    "    # This runs for each batch\n",
    "    resultList = []\n",
    "\n",
    "    # process each file in the batch\n",
    "    for f in mini_batch:\n",
    "        # Read comma-delimited data into an array\n",
    "        data = np.genfromtxt(f, delimiter=',')\n",
    "        # Reshape into a 2-dimensional array for model input\n",
    "        prediction = model.predict(data.reshape(1, -1))\n",
    "        # Append prediction to results\n",
    "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 -  Créer un pipeline avec un ParallelRunStep et ParallelRunConfig**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "# Get the batch dataset for input\n",
    "batch_data_set = ws.datasets['batch-data']\n",
    "\n",
    "# Set the output location\n",
    "output_dir = OutputFileDatasetConfig(name='inferences')\n",
    "\n",
    "# Define the parallel run step step configuration\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='batch_scripts',\n",
    "    entry_script=\"batch_scoring_script.py\",\n",
    "    mini_batch_size=\"5\",\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    environment=batch_env,\n",
    "    compute_target=aml_cluster,\n",
    "    node_count=4)\n",
    "\n",
    "# Create the parallel run step\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name='batch-score',\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[batch_data_set.as_named_input('batch_data')],\n",
    "    output=output_dir,\n",
    "    arguments=[],\n",
    "    allow_reuse=True\n",
    ")\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 - Exécuter le pipeline et récupérer la sortie de l’étape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "# Run the pipeline as an experiment\n",
    "pipeline_run = Experiment(ws, 'batch_prediction_pipeline').submit(pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=True)\n",
    "\n",
    "# Get the outputs from the first (and only) step\n",
    "prediction_run = next(pipeline_run.get_children())\n",
    "prediction_output = prediction_run.get_output_data('inferences')\n",
    "prediction_output.download(local_path='results')\n",
    "\n",
    "# Find the parallel_run_step.txt file\n",
    "for root, dirs, files in os.walk('results'):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "\n",
    "# Load and display the results\n",
    "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
    "df.columns = [\"File\", \"Prediction\"]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5 - Publication du Pipeline**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(name='Batch_Prediction_Pipeline',\n",
    "                                                   description='Batch pipeline',\n",
    "                                                   version='1.0')\n",
    "rest_endpoint = published_pipeline.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser avec un endpoint\n",
    "import requests\n",
    "response = requests.post(rest_endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"Batch_Prediction\"})\n",
    "run_id = response.json()[\"Id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planifier execution\n",
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "weekly = ScheduleRecurrence(frequency='Week', interval=1)\n",
    "pipeline_schedule = Schedule.create(ws, name='Weekly Predictions',\n",
    "                                        description='batch inferencing',\n",
    "                                        pipeline_id=published_pipeline.id,\n",
    "                                        experiment_name='Batch_Prediction',\n",
    "                                        recurrence=weekly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Recherche d' hyperparamètres\n",
    "Parametre discret :\n",
    "- qnormal\n",
    "- quniform\n",
    "- qlognormal\n",
    "- qloguniform\n",
    "\n",
    "Paramettre continu:\n",
    "- normal\n",
    "- uniforme\n",
    "- lognormal\n",
    "- loguniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Espace de recherche**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import choice, normal\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': normal(10, 3)\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Échantillonnage par grille**  GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import GridParameterSampling, choice\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': choice(0.01, 0.1, 1.0)\n",
    "              }\n",
    "              \n",
    "param_sampling = GridParameterSampling(param_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Échantillonnage par grille**  RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import RandomParameterSampling, choice, normal\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': normal(10, 3)\n",
    "              }\n",
    "              \n",
    "param_sampling = RandomParameterSampling(param_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Echantillonage Bayesien**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import BayesianParameterSampling, choice, uniform\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': uniform(0.05, 0.1)\n",
    "              }\n",
    "\n",
    "param_sampling = BayesianParameterSampling(param_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Early Stoping** Strategie Bandit  \n",
    "arrêter une exécution si la mesure de performance cible affiche des performances inférieures d’une marge spécifiée à celles de la meilleure exécution jusqu’à présent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import BanditPolicy\n",
    "early_termination_policy = BanditPolicy(slack_amount = 0.2,\n",
    "                                        evaluation_interval=1,\n",
    "                                        delay_evaluation=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Early Stoping** Strategie Arret Mediane  \n",
    "abandonne les exécutions où la mesure de performance cible est inférieure à la valeur médiane des moyennes en cours d’exécution pour toutes les exécutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import MedianStoppingPolicy\n",
    "early_termination_policy = MedianStoppingPolicy(evaluation_interval=1,\n",
    "                                                delay_evaluation=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Early Stoping** Strategie Sélection de Troncation  \n",
    "annule les x % des exécutions les moins performantes à chaque intervalle d’évaluation en fonction de la valeur de truncation_percentage que vous spécifiez pour X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import TruncationSelectionPolicy\n",
    "early_termination_policy = TruncationSelectionPolicy(truncation_percentage=10,\n",
    "                                                     evaluation_interval=1,\n",
    "                                                     delay_evaluation=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Fine Tuning Hyperparametres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Script de formation .py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import joblib\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Get regularization hyperparameter\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01)\n",
    "args = parser.parse_args()\n",
    "reg = args.reg_rate\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the training dataset\n",
    "data = run.input_datasets['training_data'].to_pandas_dataframe()\n",
    "\n",
    "# Separate features and labels, and split for training/validatiom\n",
    "X = data[['feature1','feature2','feature3','feature4']].values\n",
    "y = data['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "# Train a logistic regression model with the reg hyperparameter\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate and log accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# Save the trained model\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration et exécution d’une expérience hyperdrive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.train.hyperdrive import HyperDriveConfig, PrimaryMetricGoal\n",
    "\n",
    "# Assumes ws, script_config and param_sampling are already defined\n",
    "\n",
    "hyperdrive = HyperDriveConfig(run_config=script_config,\n",
    "                              hyperparameter_sampling=param_sampling,\n",
    "                              policy=None,\n",
    "                              primary_metric_name='Accuracy',\n",
    "                              primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                              max_total_runs=6,\n",
    "                              max_concurrent_runs=4)\n",
    "\n",
    "experiment = Experiment(workspace = ws, name = 'hyperdrive_training')\n",
    "hyperdrive_run = experiment.submit(config=hyperdrive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Surveillance et examen des exécutions d’hyperdrive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  récupérer les mesures journalisées \n",
    "for child_run in run.get_children():\n",
    "    print(child_run.id, child_run.get_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# répertorier toutes les exécutions dans l’ordre décroissant des performances\n",
    "for child_run in hyperdrive_run.get_children_sorted_by_primary_metric():\n",
    "    print(child_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupérer l’exécution la plus performante\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DP100-IdpZmbhO')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6be20d35a0fe9891e9592b51856519ffefec3add89e5783bfb8eccfc4b79debb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
